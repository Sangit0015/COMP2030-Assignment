# FUSS Platform Usability Testing Report

This report presents the results of usability testing conducted based on the *Usability Test Plan (Part 1)*. All tasks, participants, and success criteria outlined in the plan were followed. Observations and metrics reported here reflect the actual performance and experiences of participants during the testing sessions.

Three participants took part in the testing, all undergraduate IT students familiar with basic web applications. Each participant performed the six planned tasks using a live version of the system connected to the project database. The think-aloud protocol was applied, with the researcher recording verbal comments, timing each task, and noting visible errors or difficulties. A short post-test questionnaire collected SEQ ratings for each task and a final SUS score to gauge overall usability.

Each session lasted approximately twenty-five minutes. Participants were informed about the purpose of the study, provided consent, completed the set of tasks, answered the questionnaire, and participated in a brief interview. Sessions were observed and screen recordings were made to confirm task timing and completion, and quantitative data were merged with qualitative observations.

The overall completion rate across all tasks was eighty-eight percent, with an average SUS score of seventy-seven. These figures indicate good usability; however, several recurring interface issues were identified that limited efficiency or user confidence.

## Task Results

### Authentication
All participants were able to log in after one or two attempts. The backend correctly rejected invalid credentials and handled sessions securely. Participants successfully authenticated and navigated to the home page, meeting the planned success criteria.

### Profile Update
Data persistence worked correctly in the profile-update task. However, participants were unclear whether their changes had been saved, as there was no visible confirmation message. One participant refreshed the page multiple times to verify changes. A simple on-screen success message would resolve this issue.

### Skill Creation and Discovery
Two participants completed skill creation and discovery smoothly, but one struggled to locate the section for adding skills. The search functionality was functional but slow since the list reloaded entirely with each query. Navigation clarity was the primary usability concern in this task.

### Messaging
All participants successfully sent and received messages. Technical functionality was correct, though participants noted minor interface inconsistencies in conversation order display. The messaging task met its functional goals, with only small refinements suggested for clarity.

### Credit Viewing
Participants could view balances and recent transactions accurately. While functionality was correct, users requested clearer contextual labels to understand what each transaction represented.

### Missing Credit-Request Feature
Participants immediately noticed the absence of a credit-request feature and considered it essential for perceived fairness in the credit exchange system.

## Summary of Findings

The backend proved stable and responsive, with no crashes or data errors during testing. Major usability issues were related to **feedback clarity, navigation, and missing functionality** rather than functional defects. Participants praised the peer-to-peer skill sharing concept and the credit system, but all agreed that the site would benefit from clearer guidance and status indicators.

### Metrics
- **Average time per task:** under two minutes (within success thresholds)  
- **Median SEQ rating:** 5–6 on a seven-point scale (tasks perceived as easy to moderately easy)  
- **Error rate:** low; mostly minor re-entry due to lack of inline validation  

## Prioritized Usability Issues

1. **Profile update confirmation** – lack of visible success message led to uncertainty.  
2. **Skill creation and discovery navigation** – unclear pathways slowed task completion.  
3. **Minor messaging interface inconsistencies** – conversation ordering could be clearer.  
4. **Missing credit-request feature** – a critical conceptual gap affecting user expectations.  
5. **Credit transaction clarity** – labels and formatting could improve comprehension.  

## Recommendations

To address these issues while maintaining backend stability, the next iteration should:

- Introduce readable on-screen status messages for key actions.  
- Implement inline form validation to prevent common input errors.  
- Redesign navigation to make core pages and functions easier to locate.  
- Develop the missing credit-request feature.  
- Refine messaging and transaction displays for clarity and consistency.  
